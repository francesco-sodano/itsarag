{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 3: Start Coding\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this challenge, you will interact with Azure OpenAI and Phi-3 APIs using Python.\n",
    "You can use the following notebook schema and complete the code, or you can create your own notebook from scratch.\n",
    "\n",
    "The steps to complete the challenge are:\n",
    "- Play with the vanilla models\n",
    "- Bring your own data\n",
    "\n",
    "Be sure you have your python environment activated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Play with the vanilla models\n",
    "\n",
    "In this step, you need to connect to the Azure OpenAI and Phi-3 APIs using code.\n",
    "\n",
    "### Azure OpenAI API\n",
    "\n",
    "Let's start with Azure OpenAI API.\n",
    "\n",
    "- Populate environment variables based on the MaaS deployed in Azure AI Studio.\n",
    "- Provide the question as prompt (you can use questions from the first part of the challenge).\n",
    "- Create the OpenAI API client.\n",
    "- Use the OpenAI API client to generate completions.\n",
    "- Print the completions.\n",
    "- Print the number of tokens used in the prompt and the completion.\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "So that we do not commit any secrets in our Git repository we are using <a href=\"https://pypi.org/project/python-dotenv/\">python-dotenv</a> to manager our environment variables. It will also make things easier when deploying the application in Azure. \n",
    "\n",
    "At the root of this repository copy `env.sample.txt` to `.env` open `.env` and edit the variables for step 1. You can edit variables for other steps later when you get there.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, dotenv\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "# Setup environment\n",
    "AZURE_OPENAI_API_KEY = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "AZURE_OPENAI_ENDPOINT = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "AZURE_OPENAI_API_VERSION = os.getenv(\"AZURE_OPENAI_API_VERSION\")\n",
    "AZURE_OPENAI_MODEL = os.getenv(\"AZURE_OPENAI_MODEL\")\n",
    "AZURE_OPENAI_DEPLOYMENT_NAME = os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME\")\n",
    "\n",
    "# Libraries\n",
    "from openai import AzureOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the question\n",
    "\n",
    "# Create an Azure OpenAI client\n",
    "\n",
    "# Use the client to generate completions\n",
    "\n",
    "# Print the response\n",
    "\n",
    "# Get the number of tokens in the response (prompt_tokens and completion_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phi-3 API\n",
    "\n",
    "Now, let's do the same using the Phi-3 API.\n",
    "\n",
    "The steps are similar to the Azure OpenAI API.\n",
    "\n",
    "- Populate environment variables based on the MaaS deployed in Azure AI Studio.\n",
    "- Provide the question as prompt (you can use questions from the first part of the challenge)\n",
    "- Create the OpenAI API client.\n",
    "- Use the OpenAI API client to generate completions.\n",
    "- Print the completions.\n",
    "- Print the number of tokens used in the prompt and the completion.\n",
    "\n",
    "**Important:** Make sure you update your `.env` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, dotenv\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "# Setup environment\n",
    "PHI_API_KEY = os.getenv(\"PHI_API_KEY\")\n",
    "PHI_ENDPOINT = os.getenv(\"PHI_ENDPOINT\") # needs to be of format https://<deployment_name>.<location>.models.ai.azure.com\n",
    "PHI_DEPLOYMENT_NAME = os.getenv(\"PHI_DEPLOYMENT_NAME\") # not used as the client both expect the model name not deployment name\n",
    "\n",
    "# Libraries\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the question\n",
    "\n",
    "# Create an Azure OpenAI client (Phi3 is compatible with Azure OpenAI)\n",
    "\n",
    "# Use the client to generate completions\n",
    "\n",
    "# Print the response\n",
    "\n",
    "# Get the number of tokens in the response (prompt_tokens and completion_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Bring your own data\n",
    "\n",
    "After the test of the vanilla models, now it's time to bring your data into the picture.\n",
    "\n",
    "\n",
    "We will use Langchain framework and Azure AI Search for this.\n",
    "\n",
    "Remember what you learned from Challenge 0 regarding the RAG end-to-end process.\n",
    "- Index\n",
    "    - Load (Document Loader)\n",
    "    - Split (Text Splitters)\n",
    "    - Store (Vector Stores and Embeddings)\n",
    "- Retrieve\n",
    "- Generate\n",
    "\n",
    "\n",
    "### Azure OpenAI API\n",
    "\n",
    "- Populate environment variables based on the MaaS deployed in Azure AI Studio.\n",
    "- Create a Search Vector Store, the Azure Open AI embedding and the Azure Chat OpenAI objects.\n",
    "- Index : Load documents from the data source (you can use AzureBlobStorageContainerLoader)\n",
    "- Index : Split the documents in chucks (you can use the RecursiveCharacterTextSplitter)\n",
    "- Index : Store the documents in the vector store (you can use the add_documents method)\n",
    "- Retrieve: Create a retriver using the Vector Store (SimilaritySearch and top_k)\n",
    "- Generate: Use the langchain chain to generate completions (get context from retriever and format the context in single line with the question -> add the proper prompt -> send to LLM -> get structured output)\n",
    "\n",
    "**Important:** Make sure you update your `.env` file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "Here the preferred endpoint for Gpt-4o and embedding is the global one of format : \"https://< location >.api.cognitive.microsoft.com\"\n",
    "\n",
    "you need to be careful on the api-version which can be different between gpt-4o and embedding\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, dotenv\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "# ENVIRONMENT VARIABLES\n",
    "# OpenAI\n",
    "AZURE_OPENAI_API_KEY = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "AZURE_OPENAI_ENDPOINT = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "AZURE_OPENAI_API_VERSION = os.getenv(\"AZURE_OPENAI_API_VERSION\")\n",
    "AZURE_OPENAI_MODEL = os.getenv(\"AZURE_OPENAI_MODEL\")\n",
    "AZURE_OPENAI_DEPLOYMENT_NAME = os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME\")\n",
    "\n",
    "AZURE_OPENAI_EMBEDDING = os.getenv(\"AZURE_OPENAI_EMBEDDING\")\n",
    "AZURE_OPENAI_EMBEDDING_API_VERSION = os.getenv(\"AZURE_OPENAI_EMBEDDING_API_VERSION\")\n",
    "\n",
    "# Azure Search\n",
    "AZURE_SEARCH_ENDPOINT = os.getenv(\"AZURE_SEARCH_ENDPOINT\")\n",
    "AZURE_SEARCH_API_KEY = os.getenv(\"AZURE_SEARCH_API_KEY\")\n",
    "AZURE_SEARCH_INDEX = os.getenv(\"AZURE_SEARCH_INDEX\")\n",
    "\n",
    "# Azure Blob Storage\n",
    "AZURE_STORAGE_CONNECTION_STRING = os.getenv(\"AZURE_STORAGE_CONNECTION_STRING\")\n",
    "AZURE_STORAGE_CONTAINER = os.getenv(\"AZURE_STORAGE_CONTAINER\")\n",
    "\n",
    "# Import Libraries\n",
    "from langchain_openai import AzureOpenAIEmbeddings, AzureChatOpenAI\n",
    "from langchain_community.vectorstores.azuresearch import AzureSearch\n",
    "from langchain_community.document_loaders import AzureBlobStorageContainerLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the required objects\n",
    "# Azure OpenAI Embeddings\n",
    "\n",
    "# Azure Search Vector Store\n",
    "\n",
    "# Define the LLM model to use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here make sure you have enabled for BOTH Storage accounts (stitsaragdataxxx and stitsaragxxx) the Allow storage account key access in Settings -> Configuration.\n",
    "\n",
    "Here the connection string needs to be in the format : \"DefaultEndpointProtocol=https;AccountName=< nameofyourstorageaccount >;AccountKey=< key >;EndpointSuffix=core.windows.net\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index: Load the documents\n",
    "# Load the document from Azure Blob Storage (AzureBlobStorageContainerLoader)\n",
    "# Note: It can take up to 5 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index: Split (RecursiveCharacterTextSplitter - 1000 characters - 200 overlap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index: Store (add_documents)\n",
    "# It can take up to 10 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve (similarity_score_threshold - score_threshold=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate\n",
    "# Take all the result documents from the retriever and format them into a single string suitable for input into the language model.\n",
    "\n",
    "# Use the ChatPromptTemplate to define the prompt that will be sent to the model (Human) remember to include the question and the context\n",
    "\n",
    "# Define the Chain to get the answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the solution\n",
    "print(rag_chain.invoke(\"What are the revenues of Google in the year 2000?\"))\n",
    "print(rag_chain.invoke(\"What are the revenues and the operative margins of Alphabet Inc. in 2022 and how it compares with the previous year?\"))\n",
    "print(rag_chain.invoke(\"Can you compare and create a table with the revenue of Alphabet Inc., NVIDIA, MICROSOFT, APPLE and AMAZON in years 2023?\"))\n",
    "print(rag_chain.invoke(\"Did APPLE repurchase common stock in 2023? create a table of Apple repurchased stock with date, numbers of stocks and values in dollars.\"))\n",
    "print(rag_chain.invoke(\"Can you give me the Fiscal Year 2023 Highlights for Apple, Microsoft, Nvidia and Google?\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
