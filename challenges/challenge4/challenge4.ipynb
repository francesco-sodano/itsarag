{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge 4: Advanced RAG with Azure AI Document intelligence\n",
    "\n",
    "Many documents in  real scenario, are not just text, they are a combination of text, images, tables, etc. In this step, you will create a more advanced RAG application able to deal with this kind of documents.\n",
    "For this reason, you will use Azure AI Document Intelligence to extract the text, images, and tables from the documents and use them as input for the RAG model.\n",
    "\n",
    "To achieve this, we will build on top of the langchain framework enhancing the `Document Loader` and `Text Splitters` to deal with images and tables.\n",
    "In the code repositiory, you have already the enhanced version of the `Document Loader` and `Text Splitters` that you can use. They are included in two different python modules: `document_loader.py` and `text_splitters.py`.\n",
    "\n",
    "You can now use these libraries to create your advanced RAG.\n",
    "\n",
    "We provided already the libraries and the Environment variables required (you need just to populate them)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, dotenv\n",
    "dotenv.load_dotenv()\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), '../../lib')))\n",
    "\n",
    "# Setup environment\n",
    "\n",
    "# OpenAI\n",
    "AZURE_OPENAI_API_KEY = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "AZURE_OPENAI_ENDPOINT = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "AZURE_OPENAI_API_VERSION = os.getenv(\"AZURE_OPENAI_API_VERSION\")\n",
    "AZURE_OPENAI_MODEL = os.getenv(\"AZURE_OPENAI_MODEL\")\n",
    "AZURE_OPENAI_DEPLOYMENT_NAME = os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME\")\n",
    "AZURE_OPENAI_EMBEDDING = os.getenv(\"AZURE_OPENAI_EMBEDDING\")\n",
    "# Azure Search\n",
    "AZURE_SEARCH_ENDPOINT = os.getenv(\"AZURE_SEARCH_ENDPOINT\")\n",
    "AZURE_SEARCH_API_KEY = os.getenv(\"AZURE_SEARCH_API_KEY\")\n",
    "AZURE_SEARCH_INDEX = os.getenv(\"AZURE_SEARCH_INDEX\")\n",
    "# Azure AI Document Intelligence\n",
    "AZURE_DOCUMENT_INTELLIGENCE_ENDPOINT = os.getenv(\"AZURE_DOCUMENT_INTELLIGENCE_ENDPOINT\")\n",
    "AZURE_DOCUMENT_INTELLIGENCE_API_KEY = os.getenv(\"AZURE_DOCUMENT_INTELLIGENCE_API_KEY\")\n",
    "# Azure Blob Storage\n",
    "AZURE_STORAGE_CONNECTION_STRING = os.getenv(\"AZURE_STORAGE_CONNECTION_STRING\")\n",
    "AZURE_STORAGE_CONTAINER = os.getenv(\"AZURE_STORAGE_CONTAINER\")\n",
    "AZURE_STORAGE_FOLDER = os.getenv(\"AZURE_STORAGE_FOLDER\")\n",
    "\n",
    "# Import Libraries\n",
    "import os\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from azure.ai.documentintelligence.models import DocumentAnalysisFeature\n",
    "\n",
    "# Custom Libraries\n",
    "from its_a_rag.doc_intelligence import AzureAIDocumentIntelligenceLoader\n",
    "from its_a_rag import ingestion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Vector store, the embeddings client and the OpenAI Chat client\n",
    "\n",
    "Let's start creating the vector store and the embeddings client. Because we need a custom index to store the information in the way so that our retriever wil be able to get it, we have a custom function for that (create_multimodal_vectore_store).\n",
    "For the OpenAI Chat client we will simply use the one offered by langchain framework as in the Step 3 of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the index for Azure Search store and Embedding\n",
    "\n",
    "\n",
    "# Create the Azure OpenAI Chat Client\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index Phase\n",
    "\n",
    "As always the first step is to index the documents:\n",
    "the high level steps are:\n",
    "\n",
    "- Set Folder Path: Assign the local folder path to the variable folder.\n",
    "- List Files: Create a list of files in the specified folder.\n",
    "- Get Full Paths: Convert the list of file names to their full paths.\n",
    "- Iterate Over Files: Loop through each file in the list.\n",
    "    - Extract File Name: Extract the file name from the full path (this is required for the document loader).\n",
    "    - Load Document: Use AzureAIDocumentIntelligenceLoader to load the document with specified API credentials and settings (remember to use pre-built layout as model and the latest API version)\n",
    "    - Split Document: Split the loaded document using a custom advanced text splitter.\n",
    "    - Store Document: Add the processed documents to a multimodal vector store (using the add_documents method)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index\n",
    "\n",
    "# Index: Load files\n",
    "\n",
    "# Get list of files in a local folder\n",
    "\n",
    "# For each file\n",
    "    # Get the file name\n",
    "    \n",
    "    # Index : Load the file and create a document\n",
    "    \n",
    "    # Index : Split (using advanced text splitter)\n",
    "    \n",
    "    # Index : Store (add_documents)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve Phase\n",
    "\n",
    "The next step is to create a retriever for the documents based on the user query.\n",
    "You should use the following parameters:\n",
    "- Search Type: Hybrid\n",
    "- number of results: 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve (as_retriever)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Phase\n",
    "\n",
    "The final step is to generate the answer using the RAG model.\n",
    "We will create a Langchain chain with the following steps:\n",
    " - Retrieve the docs and get the image desciption if the doc matedata is an image (with get_image_description function - RunnableLambda), then pass the context and question (using RunnablePassthrough) to the next phase\n",
    " - Use the advanced multimodal Prompt function to append system messages, the context including the text, the image (if present) and the question - check RannableLambda method also here.\n",
    " - Use the OpenAI model to generate the answer\n",
    " - Parse the output and return the answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate\n",
    "\n",
    "# RAG pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the Solution\n",
    "\n",
    "You can test the solution by providing a question and checking the answer generated by the RAG model (invoke the Langchain chain).\n",
    "\n",
    "Try to get answer for the following questions:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the solution\n",
    "print(chain_multimodal_rag.invoke(\"What are the revenues of Google in the year 2000?\"))\n",
    "print(chain_multimodal_rag.invoke(\"What are the revenues and the operative margins of Alphabet Inc. in 2022 and how it compares with the previous year?\"))\n",
    "print(chain_multimodal_rag.invoke(\"Can you compare and create a table with the revenue of Alphabet Inc., NVIDIA, MICROSOFT, APPLE and AMAZON in years 2023?\"))\n",
    "print(chain_multimodal_rag.invoke(\"Did APPLE repurchase common stock in 2023? create a table of Apple repurchased stock with date, numbers of stocks and values in dollars.\"))\n",
    "print(chain_multimodal_rag.invoke(\"Can you give me the Fiscal Year 2023 Highlights for Apple, Microsoft, Nvidia and Google?\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
