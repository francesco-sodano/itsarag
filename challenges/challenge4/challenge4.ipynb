{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge 4: Advanced RAG with Azure AI Document intelligence\n",
    "\n",
    "Many documents in  real scenario, are not just text, they are a combination of text, images, tables, etc. In this step, you will create a more advanced RAG application able to deal with this kind of documents.\n",
    "For this reason, you will use Azure AI Document Intelligence to extract the text, images, and tables from the documents and use them as input for the RAG model.\n",
    "\n",
    "To achieve this, we will build on top of the langchain framework enhancing the `Document Loader` and `Text Splitters` to deal with images and tables.\n",
    "In the code repositiory, you have already the enhanced version of the `Document Loader` and `Text Splitters` that you can use. They are included in two different python modules: `doc_intelligence.py` and `ingestion.py`.\n",
    "\n",
    "You can now use these libraries to create your advanced RAG.\n",
    "\n",
    "We provided already the libraries and the Environment variables required (you need just to populate them)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, dotenv\n",
    "dotenv.load_dotenv(override=True)\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), '../../lib')))\n",
    "\n",
    "# Setup environment\n",
    "\n",
    "# OpenAI\n",
    "AZURE_OPENAI_API_KEY = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "AZURE_OPENAI_ENDPOINT = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "AZURE_OPENAI_API_VERSION = os.getenv(\"AZURE_OPENAI_API_VERSION\")\n",
    "AZURE_OPENAI_MODEL = os.getenv(\"AZURE_OPENAI_MODEL\")\n",
    "AZURE_OPENAI_DEPLOYMENT_NAME = os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME\")\n",
    "AZURE_OPENAI_EMBEDDING = os.getenv(\"AZURE_OPENAI_EMBEDDING\")\n",
    "# Azure Search\n",
    "AZURE_SEARCH_ENDPOINT = os.getenv(\"AZURE_SEARCH_ENDPOINT\")\n",
    "AZURE_SEARCH_API_KEY = os.getenv(\"AZURE_SEARCH_API_KEY\")\n",
    "# Azure AI Document Intelligence\n",
    "AZURE_DOCUMENT_INTELLIGENCE_ENDPOINT = os.getenv(\"AZURE_DOCUMENT_INTELLIGENCE_ENDPOINT\")\n",
    "AZURE_DOCUMENT_INTELLIGENCE_API_KEY = os.getenv(\"AZURE_DOCUMENT_INTELLIGENCE_API_KEY\")\n",
    "AZURE_DOCUMENT_INTELLIGENCE_API_VERSION= os.getenv(\"AZURE_DOCUMENT_INTELLIGENCE_API_VERSION\")\n",
    "# Azure Blob Storage\n",
    "AZURE_STORAGE_CONNECTION_STRING = os.getenv(\"AZURE_STORAGE_CONNECTION_STRING\")\n",
    "AZURE_STORAGE_CONTAINER = os.getenv(\"AZURE_STORAGE_CONTAINER\")\n",
    "AZURE_STORAGE_FOLDER = os.getenv(\"AZURE_STORAGE_FOLDER\")\n",
    "\n",
    "# Import Libraries\n",
    "import os\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from azure.ai.documentintelligence.models import DocumentAnalysisFeature\n",
    "\n",
    "# Custom Libraries\n",
    "from its_a_rag.doc_intelligence import AzureAIDocumentIntelligenceLoader\n",
    "from its_a_rag import ingestion\n",
    "\n",
    "# Define the questions list (if you are using your own dataset you need to change this list)\n",
    "QUESTIONS = [\n",
    "  \"What are the revenues of GOOGLE in the year 2009?\",\n",
    "  \"What are the revenues and the operative margins of ALPHABET Inc. in 2022 and how it compares with the previous year?\",\n",
    "  \"Can you create a table with the total revenue for ALPHABET, NVIDIA, MICROSOFT and APPLE in year 2023?\",\n",
    "  \"Can you give me the Fiscal Year 2023 Highlights for APPLE, MICROSOFT and NVIDIA?\",\n",
    "  \"Did APPLE repurchase common stock in 2023? create a table of APPLE repurchased stock with date, numbers of stocks and values in dollars.\",\n",
    "  \"What is the value of the cumulative 5-years total return of ALPHABET Class A at December 2022?\",\n",
    "  \"What was the price of APPLE, NVIDIA and MICROSOFT stock in 23/07/2024?\",\n",
    "  \"Can you buy 10 shares of APPLE for me?\"\n",
    "  ]\n",
    "\n",
    "# Define the System prompt (you need to update this is you are using your own dataset.)\n",
    "system_prompt = \"\"\" You are a financial assistant tasked with answering questions related to the financial results of major technology companies listed on NASDAQ, \\n\n",
    "specifically Microsoft (MSFT), Alphabet Inc. (GOOGL), Nvidia (NVDA), Apple Inc. (AAPL), and Amazon (AMZN). \\n\n",
    "if you don't find the answer in the context, just say `I don't know.`\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Vector store, the embeddings client and the OpenAI Chat client\n",
    "\n",
    "Let's start creating the vector store and the embeddings client. Because we need a custom index to store the information in the way so that our retriever wil be able to get it, we have a custom function for that (create_multimodal_vectore_store).\n",
    "For the OpenAI Chat client we will simply use the one offered by langchain framework as in the Step 3 of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the index for Azure Search store and Embedding (using the custom function create_multimodal_vectore_store)\n",
    "# NOTE: Remember to create the new index in Azure Search called \"itsarag-ch4-001\"\n",
    "",
    "# Create the Azure OpenAI Chat Client\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index Phase\n",
    "\n",
    "As always the first step is to index the documents:\n",
    "the high level steps are:\n",
    "\n",
    "- Set Folder Path: Assign the local folder path to the variable folder.\n",
    "- List Files: Create a list of files in the specified folder.\n",
    "- Get Full Paths: Convert the list of file names to their full paths.\n",
    "- Iterate Over Files: Loop through each file in the list.\n",
    "    - Extract File Name: Extract the file name from the full path (this is required for the document loader).\n",
    "    - Load Document: Use AzureAIDocumentIntelligenceLoader to load the document with specified API credentials and settings (remember to use pre-built layout as model and the latest API version)\n",
    "    - Split Document: Split the loaded document using a custom advanced text splitter.\n",
    "    - Store Document: Add the processed documents to a multimodal vector store (using the add_documents method)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index\n",
    "\n",
    "# Index: Load files\n",
    "\n",
    "# Get list of files in a local folder that start with \"2023\"\n",
    "folder = \"./data/fsi/pdf\"\n",
    "files = [f for f in os.listdir(folder) if os.path.isfile(os.path.join(folder, f)) and f.startswith(\"2023\")]\n",
    "files = [os.path.join(folder, f) for f in files]\n",
    "\n",
    "# For each file\n",
    "for file in files:\n",
    "    # Get the file name\n",
    "    pdf_file_name = file.split(\"\\\\\")[-1]\n",
    "    # Index : Load the file and create a document\n",
    "    print(\"Processing: \", file)\n",
    "",
    "    # Index : Split (using advanced text splitter from the custom library)\n",
    "",
    "    # Index : Store (add_documents from the custom library)\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve Phase\n",
    "\n",
    "The next step is to create a retriever for the documents based on the user query.\n",
    "You should use the following parameters:\n",
    "- Search Type: Hybrid\n",
    "- number of results: 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve  (as_retriever)\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Phase\n",
    "\n",
    "The final step is to generate the answer using the RAG model.\n",
    "We will create a Langchain chain with the following steps:\n",
    " - Retrieve the docs and get the image description if the doc matedata is an image (with get_image_description function - RunnableLambda), then pass the context and question (using RunnablePassthrough) to the next phase\n",
    " - Use the advanced multimodal Prompt function to append system messages, the context including the text, the image (if present) and the question - check RannableLambda method also here.\n",
    " - Use the OpenAI model to generate the answer\n",
    " - Parse the output and return the answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate\n",
    "\n",
    "# RAG pipeline\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the Solution\n",
    "\n",
    "You can test the solution by providing a question and checking the answer generated by the RAG model (invoke the Langchain chain).\n",
    "\n",
    "Try to get answer for the following questions:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the solution\n",
    "for QUESTION in QUESTIONS:\n",
    "    print(f\"QUESTION: {QUESTION}\")\n",
    "    print(chain_multimodal_rag.invoke(QUESTION))\n",
    "    print(\"--------------------------------------------------\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}